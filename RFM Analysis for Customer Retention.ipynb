{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1 – Data Handling & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Cleaning & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(r\"C:\\Users\\samiksha.Godghate\\Desktop\\Assignment\\df_2021.csv\")\n",
    "df2 = pd.read_csv(r\"C:\\Users\\samiksha.Godghate\\Desktop\\Assignment\\df_2022.csv\")\n",
    "df3 = pd.read_csv(r\"C:\\Users\\samiksha.Godghate\\Desktop\\Assignment\\df_2023.csv\")\n",
    "df4 = pd.read_csv(r\"C:\\Users\\samiksha.Godghate\\Desktop\\Assignment\\df_2024.csv\")\n",
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of DataFrames\n",
    "data_frames = [df1, df2, df3, df4]\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Print the columns of the combined DataFrame to verify\n",
    "print(data.columns)\n",
    "\n",
    "# Optionally, print the first few rows of the combined DataFrame to verify the concatenation\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "\n",
    "# Handle missing data (example: removing rows with missing values)\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct data types\n",
    "data['user_id'] = data['user_id'].astype(str)\n",
    "data['bill_id'] = data['bill_id'].astype(str)\n",
    "data['line_item_amount'] = data['line_item_amount'].astype(float)\n",
    "data['transaction_date'] = pd.to_datetime(data['transaction_date'])\n",
    "data['description'] = data['description'].astype(str)\n",
    "data['inventory_category'] = data['inventory_category'].astype(str)\n",
    "data['color'] = data['color'].astype(str)\n",
    "data['size'] = data['size'].astype(str)\n",
    "data['zone_name'] = data['zone_name'].astype(str)\n",
    "data['store_name'] = data['store_name'].astype(str)\n",
    "data['year'] = data['year'].astype(int)\n",
    "\n",
    "# Check for inconsistencies (example: negative values)\n",
    "inconsistencies = data[data['line_item_amount'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data Summary & Quality Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transactions: 1262425\n",
      "Unique Customers: 546082\n",
      "Number of Stores: 451\n",
      "Number of Products: 72234\n"
     ]
    }
   ],
   "source": [
    "# Compute basic statistics\n",
    "total_transactions = data['bill_id'].nunique()\n",
    "unique_customers = data['user_id'].nunique()\n",
    "number_of_stores = data['store_name'].nunique()\n",
    "number_of_products = data['description'].nunique()\n",
    "\n",
    "# Identify potential data quality issues\n",
    "negative_values = data[data['line_item_amount'] < 0]\n",
    "incorrect_timestamps = data[data['transaction_date'] > pd.Timestamp.now()]\n",
    "mismatched_records = data[data['year'] != data['transaction_date'].dt.year]\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total Transactions: {total_transactions}\")\n",
    "print(f\"Unique Customers: {unique_customers}\")\n",
    "print(f\"Number of Stores: {number_of_stores}\")\n",
    "print(f\"Number of Products: {number_of_products}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Basic Sales Trends**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revenue per Year:\n",
      "year\n",
      "2021    9.209819e+09\n",
      "2022    1.052845e+10\n",
      "2023    1.084503e+10\n",
      "2024    1.072138e+10\n",
      "Name: line_item_amount, dtype: float64\n",
      "Revenue per Store:\n",
      "store_name\n",
      "Store_1       16172784.0\n",
      "Store_10      49988932.0\n",
      "Store_100     60980656.0\n",
      "Store_101     54543511.0\n",
      "Store_102     10389584.0\n",
      "                ...     \n",
      "Store_95      65274603.0\n",
      "Store_96      87482067.0\n",
      "Store_97     228959094.0\n",
      "Store_98     245644694.0\n",
      "Store_99     212033152.0\n",
      "Name: line_item_amount, Length: 451, dtype: float64\n",
      "Revenue per Category:\n",
      "inventory_category\n",
      "Accessories and Innerwear              6.639641e+09\n",
      "Casual Shirts                          1.560084e+09\n",
      "Casual Trousers, Shorts and Joggers    3.041851e+09\n",
      "Denim                                  1.475253e+09\n",
      "Disregard                              4.961575e+07\n",
      "Footwear                               8.339311e+08\n",
      "Formal Shirts                          1.173314e+10\n",
      "Formal Trousers                        5.936221e+09\n",
      "Jackets                                2.422308e+09\n",
      "Others                                 1.560688e+08\n",
      "Suits                                  3.669663e+09\n",
      "Sweaters and Sweatshirts               6.033127e+08\n",
      "T-Shirts                               2.143245e+09\n",
      "Waistcoat                              5.092155e+08\n",
      "Zipper Jacket                          5.311257e+08\n",
      "Name: line_item_amount, dtype: float64\n",
      "Top Selling Store: Store_405\n"
     ]
    }
   ],
   "source": [
    "# Calculate total revenue per year, per store, and per product category\n",
    "revenue_per_year = data.groupby('year')['line_item_amount'].sum()\n",
    "revenue_per_store = data.groupby('store_name')['line_item_amount'].sum()\n",
    "revenue_per_category = data.groupby('inventory_category')['line_item_amount'].sum()\n",
    "\n",
    "# Identify the top-selling store based on revenue\n",
    "top_selling_store = revenue_per_store.idxmax()\n",
    "\n",
    "# Print results\n",
    "print(f\"Revenue per Year:\\n{revenue_per_year}\")\n",
    "print(f\"Revenue per Store:\\n{revenue_per_store}\")\n",
    "print(f\"Revenue per Category:\\n{revenue_per_category}\")\n",
    "print(f\"Top Selling Store: {top_selling_store}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2 – Exploratory Data Analysis & Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Top-Performing Products & Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Selling Products:\n",
      "description\n",
      "Trolly Bag Promo:DARK BROWN/10    146310135.0\n",
      "PROMO TRAVEL KIT#BROWN/10          89313255.0\n",
      "BP-SPECTRUM # WHITE/42             85854456.0\n",
      "RAYN MIX COLOR # MIX COLOR/43      84096552.0\n",
      "ROBERT # WHITE/43                  77669887.0\n",
      "BP-SPECTRUM # WHITE/39             75796801.0\n",
      "BP-SPECTRUM # WHITE/40             72044324.0\n",
      "New Teddy # RED/12                 71880511.0\n",
      "ROY # WHITE/43                     63173577.0\n",
      "Trolly Bag Promo:black/10          62488051.0\n",
      "Name: line_item_amount, dtype: float64\n",
      "Best Selling Categories:\n",
      "inventory_category\n",
      "Formal Shirts                          1.173314e+10\n",
      "Accessories and Innerwear              6.639641e+09\n",
      "Formal Trousers                        5.936221e+09\n",
      "Suits                                  3.669663e+09\n",
      "Casual Trousers, Shorts and Joggers    3.041851e+09\n",
      "Jackets                                2.422308e+09\n",
      "T-Shirts                               2.143245e+09\n",
      "Casual Shirts                          1.560084e+09\n",
      "Denim                                  1.475253e+09\n",
      "Footwear                               8.339311e+08\n",
      "Name: line_item_amount, dtype: float64\n",
      "Least Performing Categories:\n",
      "inventory_category\n",
      "Disregard                   4.961575e+07\n",
      "Others                      1.560688e+08\n",
      "Waistcoat                   5.092155e+08\n",
      "Zipper Jacket               5.311257e+08\n",
      "Sweaters and Sweatshirts    6.033127e+08\n",
      "Footwear                    8.339311e+08\n",
      "Denim                       1.475253e+09\n",
      "Casual Shirts               1.560084e+09\n",
      "T-Shirts                    2.143245e+09\n",
      "Jackets                     2.422308e+09\n",
      "Name: line_item_amount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify the best-selling products and categories based on revenue and purchase frequency\n",
    "best_selling_products = data.groupby('description')['line_item_amount'].sum().sort_values(ascending=False).head(10)\n",
    "best_selling_categories = data.groupby('inventory_category')['line_item_amount'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Find the least-performing categories\n",
    "least_performing_categories = data.groupby('inventory_category')['line_item_amount'].sum().sort_values(ascending=True).head(10)\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Selling Products:\\n{best_selling_products}\")\n",
    "print(f\"Best Selling Categories:\\n{best_selling_categories}\")\n",
    "print(f\"Least Performing Categories:\\n{least_performing_categories}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Seasonality & Demand Trends**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Sales:\n",
      "month\n",
      "1     3.911686e+09\n",
      "2     3.337003e+09\n",
      "3     2.869986e+09\n",
      "4     3.647591e+09\n",
      "5     2.400541e+09\n",
      "6     2.767673e+09\n",
      "7     2.741685e+09\n",
      "8     2.483317e+09\n",
      "9     2.270645e+09\n",
      "10    4.537352e+09\n",
      "11    6.125653e+09\n",
      "12    4.211546e+09\n",
      "Name: line_item_amount, dtype: float64\n",
      "Peak Shopping Month: 11\n"
     ]
    }
   ],
   "source": [
    "# Identify monthly or yearly sales patterns\n",
    "data['month'] = data['transaction_date'].dt.month\n",
    "monthly_sales = data.groupby('month')['line_item_amount'].sum()\n",
    "\n",
    "# Detect peak shopping periods\n",
    "peak_month = monthly_sales.idxmax()\n",
    "\n",
    "# Print results\n",
    "print(f\"Monthly Sales:\\n{monthly_sales}\")\n",
    "print(f\"Peak Shopping Month: {peak_month}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Customer Purchase Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Retention Rate: 0.4307576517812343\n",
      "Average Transaction Value per Customer: 75638.23230485531\n",
      "High Value Customers:\n",
      "user_id\n",
      "0822007f39    3.079317e+09\n",
      "3f19267b63    2.047936e+07\n",
      "9782189115    1.433528e+07\n",
      "a5e7a68eaf    1.144159e+07\n",
      "0e55c85966    1.099358e+07\n",
      "                  ...     \n",
      "e4cc9f54a6    1.618200e+05\n",
      "38ea691d96    1.618200e+05\n",
      "cd01296b98    1.618200e+05\n",
      "dd0a23f88c    1.618200e+05\n",
      "5a99f2e1f5    1.618200e+05\n",
      "Name: line_item_amount, Length: 54608, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Analyze repeat purchases and customer retention rates\n",
    "repeat_purchases = data.groupby('user_id')['bill_id'].nunique()\n",
    "customer_retention = (repeat_purchases > 1).mean()\n",
    "\n",
    "# Calculate the average transaction value per customer\n",
    "average_transaction_value = data.groupby('user_id')['line_item_amount'].sum().mean()\n",
    "\n",
    "# Identify high-value customers (top 10% based on spending)\n",
    "high_value_customers = data.groupby('user_id')['line_item_amount'].sum().sort_values(ascending=False).head(int(0.1 * unique_customers))\n",
    "\n",
    "# Print results\n",
    "print(f\"Customer Retention Rate: {customer_retention}\")\n",
    "print(f\"Average Transaction Value per Customer: {average_transaction_value}\")\n",
    "print(f\"High Value Customers:\\n{high_value_customers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Preliminary Business Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Business Recommendations : \n",
    "\n",
    "1. Boost Sales of Popular Products\n",
    "- Promote top-selling items like \"Trolly Bag Promo\" and \"BP-SPECTRUM\" through discounts and bundle deals.  \n",
    "- Advertise these products more to attract more buyers.\n",
    "\n",
    "2. Stock More of Best-Selling Categories\n",
    "- Ensure Formal Shirts and Accessories & Innerwear are always available since they generate the most revenue.  \n",
    "- Adjust prices based on demand and seasons to maximize profits.\n",
    "\n",
    "3. Improve Low-Selling Categories\n",
    "- Identify reasons why Waistcoats and Zipper Jackets have low sales—price, design, or quality.  \n",
    "- Introduce new styles, collaborate with influencers, or offer discounts to boost interest.\n",
    "\n",
    "4. Engage with Customers More\n",
    "- Provide special discounts to loyal customers to encourage repeat purchases.  \n",
    "- Use emails and social media to promote new arrivals and limited-time offers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3 – Advanced Business Strategy & Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Customer Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Segmentation:\n",
      "         line_item_amount        bill_id  transaction_date\n",
      "segment                                                   \n",
      "0            4.939243e+04       1.862162          4.757375\n",
      "1            3.079317e+09  116762.000000     299532.000000\n",
      "2            4.600893e+05       6.561861         26.530803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Identify distinct customer groups based on their purchase behavior\n",
    "customer_data = data.groupby('user_id').agg({\n",
    "    'line_item_amount': 'sum',\n",
    "    'bill_id': 'nunique',\n",
    "    'transaction_date': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Use KMeans for customer segmentation\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(customer_data[['line_item_amount', 'bill_id', 'transaction_date']])\n",
    "customer_data['segment'] = kmeans.labels_\n",
    "\n",
    "# Calculate the mean for numeric columns only\n",
    "segment_means = customer_data.groupby('segment')[['line_item_amount', 'bill_id', 'transaction_date']].mean()\n",
    "\n",
    "# Print segmentation results\n",
    "print(f\"Customer Segmentation:\\n{segment_means}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id  line_item_amount  bill_id  transaction_date  segment\n",
      "0  000026db73           10780.0        1                 2        0\n",
      "1  00002dce10            2795.0        1                 1        0\n",
      "2  00003dc788            9490.0        1                 1        0\n",
      "3  00004e71b9            9184.0        1                 1        0\n",
      "4  00005db42b           35425.0        1                 5        0\n"
     ]
    }
   ],
   "source": [
    "print(customer_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   segment     recency      frequency      monetary\n",
      "0        0  649.612186       1.862162  4.939243e+04\n",
      "1        1   38.000000  116762.000000  3.079317e+09\n",
      "2        2  430.583504       6.561861  4.600893e+05\n"
     ]
    }
   ],
   "source": [
    "# Calculate RFM metrics\n",
    "rfm = data.groupby('user_id').agg({\n",
    "    'transaction_date': lambda x: (pd.to_datetime('today') - x.max()).days,  # Recency\n",
    "    'bill_id': 'nunique',  # Frequency\n",
    "    'line_item_amount': 'sum'  # Monetary Value\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['user_id', 'recency', 'frequency', 'monetary']\n",
    "\n",
    "# K-means clustering to segment customers\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(rfm[['recency', 'frequency', 'monetary']])\n",
    "rfm['segment'] = kmeans.labels_\n",
    "\n",
    "# Analyze segments\n",
    "segment_analysis = rfm.groupby('segment').agg({\n",
    "    'recency': 'mean',\n",
    "    'frequency': 'mean',\n",
    "    'monetary': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(segment_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Segmentation Analysis\n",
    "\n",
    "Segment 0:\n",
    "- Persistency ChangeRecency:Persistency Change 649.61 days\n",
    "- Persistency ChangeFrequency:Persistency Change 1.86 transactions\n",
    "- Persistency ChangeMonetary:Persistency Change ₹49,392.43\n",
    "- Persistency ChangeCharacteristics:Persistency Change These customers have not made a purchase in a long time, make very few transactions, and spend a relatively low amount.\n",
    "- Persistency ChangeEngagement Strategy:Persistency Change\n",
    "  - Persistency ChangeRe-engagement Campaign:Persistency Change Target these customers with re-engagement emails or SMS, offering exclusive discounts or promotions to encourage them to return.\n",
    "  - Persistency ChangePersonalized Recommendations:Persistency Change Use their past purchase data to recommend products they might be interested in.\n",
    "  - Persistency ChangeLoyalty Points:Persistency Change Offer loyalty points for their next purchase to incentivize a return visit.\n",
    "\n",
    "Segment 1:\n",
    "- Persistency ChangeRecency:Persistency Change 38.00 days\n",
    "- Persistency ChangeFrequency:Persistency Change 116,762.00 transactions\n",
    "- Persistency ChangeMonetary:Persistency Change ₹3,079,317,000\n",
    "- Persistency ChangeCharacteristics:Persistency Change These are high-value customers who make frequent purchases and spend a significant amount.\n",
    "- Persistency ChangeEngagement Strategy:Persistency Change\n",
    "  - Persistency ChangeVIP Treatment:Persistency Change Provide exclusive perks, early access to new products, and personalized customer service.\n",
    "  - Persistency ChangeTiered Loyalty Program:Persistency Change Implement a tiered loyalty system where these customers receive the highest tier benefits, such as larger discounts, free shipping, and special events.\n",
    "  - Persistency ChangeFeedback Loop:Persistency Change Regularly seek their feedback to understand their needs better and tailor offerings accordingly.\n",
    "\n",
    "Segment 2:\n",
    "- Persistency ChangeRecency:Persistency Change 430.58 days\n",
    "- Persistency ChangeFrequency:Persistency Change 6.56 transactions\n",
    "- Persistency ChangeMonetary:Persistency Change ₹460,089.30\n",
    "- Persistency ChangeCharacteristics:Persistency Change These customers make more frequent purchases than Segment 0 but still have a relatively long recency and moderate spending.\n",
    "- Persistency ChangeEngagement Strategy:Persistency Change\n",
    "  - Persistency ChangeRegular Engagement:Persistency Change Send regular newsletters and updates about new products, sales, and promotions.\n",
    "  - Persistency ChangeSeasonal Offers:Persistency Change Target them with seasonal offers and discounts to encourage more frequent purchases.\n",
    "  - Persistency ChangeLoyalty Rewards:Persistency Change Offer loyalty rewards for repeat purchases to incentivize continued engagement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Loyalty Program Strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data-Driven Approach to Increase Customer Retention:-\n",
    "-  Segment Analysis:  Use the segmentation data to tailor loyalty programs specifically for each segment.\n",
    "-  Personalized Offers:  Leverage customer purchase history to offer personalized discounts and recommendations.\n",
    "-  Engagement Metrics:  Track engagement metrics such as open rates, click-through rates, and conversion rates to refine the loyalty program continuously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tiered Loyalty System Benefits:- \n",
    "-  Discounts for High Spenders:  Offer increasing discounts or cashback for customers who spend more.\n",
    "-  Exclusive Perks:  Provide exclusive perks like early access to sales, special events, and personalized shopping experiences for high-tier customers.\n",
    "-  Points System:  Implement a points system where customers earn points for every purchase, which can be redeemed for discounts or free products.\n",
    "-  Engagement Rewards:  Reward customers for engaging with the brand, such as writing reviews, referring friends, or participating in surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Predictive Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictive Modeling Accuracy: 0.16404955272530833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Encode categorical target variable\n",
    "label_encoder = LabelEncoder()\n",
    "data['inventory_category_encoded'] = label_encoder.fit_transform(data['inventory_category'])\n",
    "\n",
    "# Aggregate past purchase trends\n",
    "customer_purchase_history = data.groupby('user_id').agg({\n",
    "    'inventory_category_encoded': lambda x: x.value_counts().index[0]\n",
    "}).reset_index()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = customer_purchase_history.drop(columns=['inventory_category_encoded'])\n",
    "X['user_id'] = X['user_id'].astype('category').cat.codes\n",
    "\n",
    "y = customer_purchase_history['inventory_category_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Train a predictive model\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the next likely purchase\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print predictive modeling results\n",
    "print(f\"Predictive Modeling Accuracy: {model.score(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id    predicted_next_purchase\n",
      "0        12668fb1ff              Formal Shirts\n",
      "11       059e07d4ed              Formal Shirts\n",
      "18       65f8d6a6c3  Accessories and Innerwear\n",
      "29       4297fe4bad            Formal Trousers\n",
      "30       8947a94292              Formal Shirts\n",
      "...             ...                        ...\n",
      "3499985  04b32f18a3                      Suits\n",
      "3499988  7b2a72f742                      Suits\n",
      "3499991  8f7ed3765a                    Jackets\n",
      "3499995  034800c0c7                  Waistcoat\n",
      "3499997  a2c78d3acb              Formal Shirts\n",
      "\n",
      "[546082 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_data = data[['user_id']].drop_duplicates()\n",
    "prediction_data['user_id'] = prediction_data['user_id'].astype('category').cat.codes\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(prediction_data)\n",
    "\n",
    "# Convert the encoded predictions back to the original categories\n",
    "predicted_categories = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "prediction_results = pd.DataFrame({\n",
    "    'user_id': data['user_id'].drop_duplicates(),\n",
    "    'predicted_next_purchase': predicted_categories\n",
    "})\n",
    "\n",
    "# Print the prediction results\n",
    "print(prediction_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Predictive Model Accuracy\n",
    "\n",
    "An accuracy of 16.4% is relatively low. Here are some steps to improve the model's accuracy:\n",
    "\n",
    "  1. Feature Engineering\n",
    "-  Add More Features : Include additional relevant features.\n",
    "-  Feature Interactions : Create interaction terms between features.\n",
    "\n",
    "  2. Handling Imbalanced Data\n",
    "-  Resampling : Oversample the minority class or undersample the majority class.\n",
    "-  Class Weights : Adjust class weights in the model.\n",
    "\n",
    "  3. Model Selection\n",
    "-  Try Different Algorithms : Experiment with Gradient Boosting, XGBoost, LightGBM, or neural networks.\n",
    "-  Hyperparameter Tuning : Use Grid Search or Random Search.\n",
    "\n",
    "  4. Ensemble Methods\n",
    "-  Stacking : Combine predictions from multiple models.\n",
    "-  Bagging and Boosting : Use techniques like Random Forest or Gradient Boosting.\n",
    "\n",
    "  5. Cross-Validation\n",
    "-  K-Fold Cross-Validation : Ensure consistent performance across different data subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Innovative Business Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One unique insight that could significantly impact business growth is identifying and targeting high-value customers who are likely to make repeat purchases. By focusing on these customers, the business can increase customer lifetime value (CLV) and drive sustainable growth.\n",
    "\n",
    "1.  Customer Segmentation:\n",
    "    - Segment customers based on their purchase history, frequency, and value.\n",
    "    - Identify high-value segments and tailor marketing strategies to retain and upsell to these customers.\n",
    "\n",
    "2.  Dynamic Pricing:\n",
    "    - Implement dynamic pricing strategies based on customer segments and predicted purchase behavior.\n",
    "    - Offer personalized discounts to high-value customers to encourage repeat purchases.\n",
    "\n",
    "3.  Inventory Optimization:\n",
    "    - Use predictive insights to optimize inventory levels for popular products.\n",
    "    - Ensure that high-demand products are always in stock to avoid lost sales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
